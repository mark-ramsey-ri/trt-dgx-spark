# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# TensorRT-LLM DGX Spark Cluster Configuration
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Copy this file to config.local.env and customize for your environment.
# config.local.env is gitignored and won't be committed.
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

# ┌─────────────────────────────────────────────────────────────────────────┐
# │ Model Configuration                                                     │
# └─────────────────────────────────────────────────────────────────────────┘

# Model to serve (HuggingFace model ID or local path)
# Recommended models for DGX Spark 2-node cluster:
#   - nvidia/Qwen3-235B-A22B-FP4        (Qwen3 235B FP4 quantized)
#   - openai/gpt-oss-120b               (GPT-OSS 120B MoE)
#   - openai/gpt-oss-20b                (GPT-OSS 20B MoE)
#   - meta-llama/Llama-3.3-70B-Instruct
#   - Qwen/Qwen2.5-72B-Instruct
MODEL="${MODEL:-nvidia/Qwen3-235B-A22B-FP4}"

# Tensor parallelism size (total GPUs across all nodes)
# For 2x DGX Spark (1 GPU each): TP=2 splits model across both GPUs
TENSOR_PARALLEL="${TENSOR_PARALLEL:-2}"

# Number of nodes in the cluster
NUM_NODES="${NUM_NODES:-2}"

# Maximum batch size
MAX_BATCH_SIZE="${MAX_BATCH_SIZE:-4}"

# Maximum number of tokens (context window)
MAX_NUM_TOKENS="${MAX_NUM_TOKENS:-32768}"

# ┌─────────────────────────────────────────────────────────────────────────┐
# │ Docker Configuration                                                    │
# └─────────────────────────────────────────────────────────────────────────┘

# TensorRT-LLM Docker image
# Multi-node: nvcr.io/nvidia/tensorrt-llm/release:1.0.0rc3
# Single GPU dev: nvcr.io/nvidia/tensorrt-llm/release:spark-single-gpu-dev
TRT_IMAGE="${TRT_IMAGE:-nvcr.io/nvidia/tensorrt-llm/release:1.2.0rc4}"

# Container names
HEAD_CONTAINER_NAME="${HEAD_CONTAINER_NAME:-trtllm-head}"
WORKER_CONTAINER_NAME="${WORKER_CONTAINER_NAME:-trtllm-worker}"

# Shared memory size for Docker containers
SHM_SIZE="${SHM_SIZE:-32g}"

# ┌─────────────────────────────────────────────────────────────────────────┐
# │ Worker Node Configuration (for multi-node clusters)                     │
# └─────────────────────────────────────────────────────────────────────────┘

# WORKER_IPS: Space-separated list of worker InfiniBand IPs
# The start_cluster.sh script will SSH to these nodes to start workers.
# Example for 2-node cluster: WORKER_IPS="169.254.216.8"
# Example for 3-node cluster: WORKER_IPS="169.254.216.8 169.254.217.9"
# Find worker IPs: ibdev2netdev && ip addr show <interface>
WORKER_IPS="${WORKER_IPS:-}"

# Username for SSH to worker nodes (default: current user)
WORKER_USER="${WORKER_USER:-$(whoami)}"

# ┌─────────────────────────────────────────────────────────────────────────┐
# │ Network Configuration                                                   │
# └─────────────────────────────────────────────────────────────────────────┘

# Port for TensorRT-LLM HTTP API
TRT_PORT="${TRT_PORT:-8355}"

# HEAD_IP: InfiniBand IP of the head node
# Leave empty for auto-detection (recommended)
HEAD_IP="${HEAD_IP:-}"

# ┌─────────────────────────────────────────────────────────────────────────┐
# │ Storage Configuration                                                   │
# └─────────────────────────────────────────────────────────────────────────┘

# HuggingFace cache directory on host
# Models are stored here and shared via bind mount
HF_CACHE="${HF_CACHE:-/raid/hf-cache}"

# Tiktoken encodings directory (for GPT-OSS models)
TIKTOKEN_DIR="${TIKTOKEN_DIR:-${HOME}/tiktoken_encodings}"

# ┌─────────────────────────────────────────────────────────────────────────┐
# │ Authentication                                                          │
# └─────────────────────────────────────────────────────────────────────────┘

# HuggingFace token for gated models (e.g., Llama)
# Set via: export HF_TOKEN=hf_xxx
HF_TOKEN="${HF_TOKEN:-}"

# ┌─────────────────────────────────────────────────────────────────────────┐
# │ TensorRT-LLM Server Options                                             │
# └─────────────────────────────────────────────────────────────────────────┘

# Backend for TensorRT-LLM (pytorch recommended for DGX Spark)
TRT_BACKEND="${TRT_BACKEND:-pytorch}"

# GPU memory fraction for KV cache (0.0-1.0)
GPU_MEMORY_FRACTION="${GPU_MEMORY_FRACTION:-0.90}"

# Trust remote code (required for some HuggingFace models)
TRUST_REMOTE_CODE="${TRUST_REMOTE_CODE:-true}"

# Enable CUDA graph padding for better performance
CUDA_GRAPH_PADDING="${CUDA_GRAPH_PADDING:-true}"

# Disable overlap scheduler (recommended for multi-node)
DISABLE_OVERLAP_SCHEDULER="${DISABLE_OVERLAP_SCHEDULER:-true}"

# Additional TensorRT-LLM arguments
EXTRA_ARGS="${EXTRA_ARGS:-}"

# ┌─────────────────────────────────────────────────────────────────────────┐
# │ NCCL / InfiniBand Configuration                                         │
# └─────────────────────────────────────────────────────────────────────────┘

# NCCL debug level (INFO, WARN, TRACE)
NCCL_DEBUG="${NCCL_DEBUG:-INFO}"

# InfiniBand HCA devices (auto-detected if empty)
# For DGX Spark, typically: rocep1s0f1,roceP2p1s0f1
# Leave empty for auto-detection
NCCL_IB_HCA="${NCCL_IB_HCA:-}"

# Network interface for NCCL/GLOO (auto-detected if empty)
# For DGX Spark, typically: enp1s0f1np1
NCCL_SOCKET_IFNAME="${NCCL_SOCKET_IFNAME:-}"
GLOO_SOCKET_IFNAME="${GLOO_SOCKET_IFNAME:-}"

# Disable InfiniBand (set to 1 if using Ethernet only)
NCCL_IB_DISABLE="${NCCL_IB_DISABLE:-0}"

# GPUDirect RDMA level (0-5, higher = more aggressive)
NCCL_NET_GDR_LEVEL="${NCCL_NET_GDR_LEVEL:-5}"

# NCCL timeout in seconds (default 20 min for large model loading)
NCCL_TIMEOUT="${NCCL_TIMEOUT:-1200}"
