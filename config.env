# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# TensorRT-LLM DGX Spark Cluster Configuration
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Copy this file to config.local.env and customize for your environment.
# config.local.env is gitignored and won't be committed.
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

# ┌─────────────────────────────────────────────────────────────────────────┐
# │ Model Configuration                                                     │
# └─────────────────────────────────────────────────────────────────────────┘

# Model to serve (HuggingFace model ID or local path)
# Available models (matching vLLM/SGLang):
#   Large (needs 2-node TP=2):
#     - openai/gpt-oss-120b               ~80GB+  MoE reasoning model
#     - meta-llama/Llama-3.1-70B-Instruct ~65GB   High quality (needs HF token)
#     - Qwen/Qwen2.5-72B-Instruct         ~70GB   High quality
#     - mistralai/Mixtral-8x7B-Instruct-v0.1 ~45GB MoE, fast
#   Medium (single node OK):
#     - Qwen/Qwen2.5-32B-Instruct         ~30GB   Strong mid-size
#     - google/gemma-2-27b-it             ~24GB   Strong (needs HF token)
#     - openai/gpt-oss-20b                ~16-20GB MoE, fast
#     - microsoft/phi-4                   ~14-16GB Small but smart
#     - Qwen/Qwen2.5-14B-Instruct         ~14GB   Fast
#     - mistralai/Mistral-Nemo-Instruct-2407 ~12GB 128k context
#   Small (very fast):
#     - meta-llama/Llama-3.1-8B-Instruct  ~8GB    (needs HF token)
#     - Qwen/Qwen2.5-7B-Instruct          ~7GB    Very fast
#     - mistralai/Mistral-7B-Instruct-v0.3 ~7GB   Very fast
MODEL="${MODEL:-Qwen/Qwen2.5-7B-Instruct}"

# Tensor parallelism size (total GPUs across all nodes)
# For 2x DGX Spark (1 GPU each): TP=2 splits model across both GPUs
TENSOR_PARALLEL="${TENSOR_PARALLEL:-2}"

# Number of nodes in the cluster
NUM_NODES="${NUM_NODES:-2}"

# Maximum batch size
MAX_BATCH_SIZE="${MAX_BATCH_SIZE:-4}"

# Maximum number of tokens (context window)
MAX_NUM_TOKENS="${MAX_NUM_TOKENS:-32768}"

# ┌─────────────────────────────────────────────────────────────────────────┐
# │ Docker Configuration                                                    │
# └─────────────────────────────────────────────────────────────────────────┘

# TensorRT-LLM Docker image
# Multi-node: nvcr.io/nvidia/tensorrt-llm/release:1.0.0rc3
# Single GPU dev: nvcr.io/nvidia/tensorrt-llm/release:spark-single-gpu-dev
TRT_IMAGE="${TRT_IMAGE:-nvcr.io/nvidia/tensorrt-llm/release:1.2.0rc4}"

# Container names
HEAD_CONTAINER_NAME="${HEAD_CONTAINER_NAME:-trtllm-head}"
WORKER_CONTAINER_NAME="${WORKER_CONTAINER_NAME:-trtllm-worker}"

# Shared memory size for Docker containers
SHM_SIZE="${SHM_SIZE:-32g}"

# ┌─────────────────────────────────────────────────────────────────────────┐
# │ Worker Node Configuration (for multi-node clusters)                     │
# └─────────────────────────────────────────────────────────────────────────┘
#
# Two IP addresses are needed for each worker:
#   1. WORKER_HOST  - Standard Ethernet IP for SSH access (e.g., 192.168.x.x)
#   2. WORKER_IB_IP - High-speed network IP for NCCL communication
#
# How to find these IPs on the worker node:
#   - Ethernet IP:   hostname -I | awk '{print $1}'   (or check your network config)
#   - High-speed IP: ibdev2netdev && ip addr show <ib-interface> | grep "inet "
#     (May be 169.254.x.x link-local, or a routable IP depending on your network config)
#

# WORKER_HOST: IP address for SSH access to worker node
# REQUIRED for multi-node setup! Find it on worker: hostname -I | awk '{print $1}'
# Example: 192.168.7.111
WORKER_HOST="${WORKER_HOST:-}"

# WORKER_IB_IP: High-speed network IP for NCCL/GPU communication
# This is the IP on the InfiniBand/high-speed interface for RDMA transfers
# Used for: Docker Swarm overlay network, MPI, NCCL tensor communication
# Note: May be link-local (169.254.x.x) or routable depending on your network
WORKER_IB_IP="${WORKER_IB_IP:-}"

# Username for SSH to worker nodes (default: current user)
WORKER_USER="${WORKER_USER:-$(whoami)}"

# ┌─────────────────────────────────────────────────────────────────────────┐
# │ Network Configuration                                                   │
# └─────────────────────────────────────────────────────────────────────────┘

# Port for TensorRT-LLM HTTP API
TRT_PORT="${TRT_PORT:-8355}"

# HEAD_IP: InfiniBand IP of the head node
# Leave empty for auto-detection (recommended)
HEAD_IP="${HEAD_IP:-}"

# ┌─────────────────────────────────────────────────────────────────────────┐
# │ Storage Configuration                                                   │
# └─────────────────────────────────────────────────────────────────────────┘

# HuggingFace cache directory on host
# Models are stored here and shared via bind mount
HF_CACHE="${HF_CACHE:-/raid/hf-cache}"

# Tiktoken encodings directory (for GPT-OSS models)
TIKTOKEN_DIR="${TIKTOKEN_DIR:-${HOME}/tiktoken_encodings}"

# ┌─────────────────────────────────────────────────────────────────────────┐
# │ Authentication                                                          │
# └─────────────────────────────────────────────────────────────────────────┘

# HuggingFace token for gated models (e.g., Llama)
# Set via: export HF_TOKEN=hf_xxx
HF_TOKEN="${HF_TOKEN:-}"

# ┌─────────────────────────────────────────────────────────────────────────┐
# │ TensorRT-LLM Server Options                                             │
# └─────────────────────────────────────────────────────────────────────────┘

# Backend for TensorRT-LLM (pytorch recommended for DGX Spark)
TRT_BACKEND="${TRT_BACKEND:-pytorch}"

# GPU memory fraction for KV cache (0.0-1.0)
GPU_MEMORY_FRACTION="${GPU_MEMORY_FRACTION:-0.90}"

# Trust remote code (required for some HuggingFace models)
TRUST_REMOTE_CODE="${TRUST_REMOTE_CODE:-true}"

# Enable CUDA graph padding for better performance
CUDA_GRAPH_PADDING="${CUDA_GRAPH_PADDING:-true}"

# Disable overlap scheduler (recommended for multi-node)
DISABLE_OVERLAP_SCHEDULER="${DISABLE_OVERLAP_SCHEDULER:-true}"

# Additional TensorRT-LLM arguments
EXTRA_ARGS="${EXTRA_ARGS:-}"

# ┌─────────────────────────────────────────────────────────────────────────┐
# │ NCCL / InfiniBand Configuration                                         │
# └─────────────────────────────────────────────────────────────────────────┘

# NCCL debug level (INFO, WARN, TRACE)
NCCL_DEBUG="${NCCL_DEBUG:-INFO}"

# InfiniBand HCA devices (auto-detected if empty)
# For DGX Spark, typically: rocep1s0f1,roceP2p1s0f1
# Leave empty for auto-detection
NCCL_IB_HCA="${NCCL_IB_HCA:-}"

# Network interface for NCCL/GLOO (auto-detected if empty)
# For DGX Spark, typically: enp1s0f1np1
NCCL_SOCKET_IFNAME="${NCCL_SOCKET_IFNAME:-}"
GLOO_SOCKET_IFNAME="${GLOO_SOCKET_IFNAME:-}"

# Disable InfiniBand (set to 1 if using Ethernet only)
NCCL_IB_DISABLE="${NCCL_IB_DISABLE:-0}"

# GPUDirect RDMA level (0-5, higher = more aggressive)
NCCL_NET_GDR_LEVEL="${NCCL_NET_GDR_LEVEL:-5}"

# NCCL timeout in milliseconds (default 20 min for large model loading)
NCCL_TIMEOUT="${NCCL_TIMEOUT:-1200000}"
